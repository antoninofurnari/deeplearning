{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df88a552",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning Laboratories\n",
    "\n",
    "<a href=\"https://creativecommons.org/licenses/by-nc/4.0/\">\n",
    "  <img src=\"https://licensebuttons.net/l/by-nc/4.0/88x31.png\" alt=\"CC BY-NC 4.0\" style=\"vertical-align:middle;\">\n",
    "</a>\n",
    "<span style=\"vertical-align:middle; margin-left:8px;\">\n",
    "  <strong>CC BYâ€‘NC 4.0</strong> â€” nonâ€‘commercial use with attribution.  \n",
    "  Full terms at the link.\n",
    "</span>\n",
    "\n",
    "<br>\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
    "https://colab.research.google.com/github/antoninofurnari/deeplearning/blob/main/notebooks/03-03-26-intro-laboratories.ipynb\n",
    ")\n",
    "\n",
    "**Course:** Deep Learning: Advanced Models and Methods\n",
    "\n",
    "**Objective:**\n",
    "Welcome to the course! Today, we aren't just checking our environment; we are establishing the workflow for the entire semester. Deep Learning is as much about engineering discipline as it is about mathematics.\n",
    "\n",
    "**In this lab, we will:**\n",
    "1.  **Understand the Notebook Philosophy:** Learn the visual language of these labs.\n",
    "2.  **Verify the Forge:** Ensure PyTorch and CUDA are communicating on Google Colab.\n",
    "3.  **Connect the Logger:** Initialize Weights & Biases (W&B) for experiment tracking.\n",
    "4.  **The Cluster Guide:** A reference guide for moving from Colab to the University Cluster.\n",
    "5.  **First Strike:** Train your first PyTorch Lightning model (MLP on MNIST) and log it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee1d7f1",
   "metadata": {},
   "source": [
    "## The Rules of Engagement\n",
    "\n",
    "Throughout this course, we will use specific visual cues to guide your learning. Please familiarize yourself with this legend:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>â˜‘ï¸ TODO</b> This box provides you instruction on a TODO that should be completed in the code cells below.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>ğŸ›‘ Checkpoint</b> The following cells provide you instruction on what you should observe if you run a given cell after having implemented your TODOs. This serve as a self-check to assess potential bugs or errors early.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>ğŸ“ Reflection</b> You are required to write a short reflection on your results.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>ğŸ Milestone</b> This signal that you completed a significant part of the notebook and reached a milestone.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5529f1",
   "metadata": {},
   "source": [
    "Throughout the course, we will use Weight and Biases for logging: http://wandb.ai/\n",
    "\n",
    "If you use colab, you will need to install it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d58c351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL | PURPOSE: Install necessary libraries\n",
    "# We need PyTorch Lightning for structure and WandB for logging.\n",
    "# The 'quiet' flag reduces the log noise.\n",
    "!pip install lightning wandb -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0189409b",
   "metadata": {},
   "source": [
    "## **Part 1: The Local Forge (Colab Setup)**\n",
    "\n",
    "Google Colab provides us with a free (but time-limited) GPU. Let's inspect what hardware we were assigned and ensure PyTorch can see it.\n",
    "\n",
    "We also use PyTorch and Lightning during the course. Let's check their versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2bffdf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.6.0+cu124\n",
      "Lightning Version: 2.6.0\n",
      "âœ… Success! CUDA is available.\n",
      "   GPU Name: Tesla V100-PCIE-16GB\n",
      "   Capability: (7, 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CELL | PURPOSE: Verify Environment\n",
    "import torch\n",
    "import lightning as L\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Lightning Version: {L.__version__}\")\n",
    "\n",
    "# Check GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ… Success! CUDA is available.\")\n",
    "    print(f\"   GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Capability: {torch.cuda.get_device_capability(0)}\")\n",
    "else:\n",
    "    print(\"âš ï¸ WARNING: You are running on CPU. Go to Runtime -> Change runtime type -> Hardware accelerator -> T4 GPU.\")\n",
    "\n",
    "# Set a global seed for reproducibility (Crucial for scientific rigor)\n",
    "L.seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6cf6b8",
   "metadata": {},
   "source": [
    "Let's now login to wandb:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba8a2dca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CELL | PURPOSE: Login to Weights & Biases\n",
    "# You will be prompted to paste your API key from https://wandb.ai/authorize\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb031b45",
   "metadata": {},
   "source": [
    "## **Part 2: Lightning Strike (Introduction to PyTorch Lightning)**\n",
    "\n",
    "We use **PyTorch Lightning** to organize our code. It separates the *science* (the model architecture) from the *engineering* (the training loop).\n",
    "\n",
    "**The Task:** Train a simple Multi-Layer Perceptron (MLP) to classify handwritten digits (MNIST).\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <b>â˜‘ï¸ TODO:</b> In the cell below, fill in the missing code to define the optimizer.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf0b4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model Class Defined Successfully.\n"
     ]
    }
   ],
   "source": [
    "# CELL | PURPOSE: Define the LightningModule\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class MNISTClassifier(L.LightningModule):\n",
    "    def __init__(self, hidden_dim=128, learning_rate=1e-3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters() # Logs params to W&B automatically\n",
    "\n",
    "        # 1. Define Architecture (Simple MLP)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28 * 28, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 10) # 10 classes for digits 0-9\n",
    "        )\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # The \"Magic\" Loop: No need to manual zero_grad or backward()\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        # Log to W&B\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "\n",
    "        # Log validation metrics\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        self.log(\"test_acc\", acc)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # TODO: Define the Adam optimizer using self.learning_rate\n",
    "        # Hint: Use torch.optim.Adam\n",
    "        # < YOUR CODE HERE >\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        # </ YOUR CODE HERE >\n",
    "        return optimizer\n",
    "\n",
    "print(\"âœ… Model Class Defined Successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478ffe28",
   "metadata": {},
   "source": [
    "### **Data Preparation**\n",
    "Lightning handles the training loop, but we still need standard PyTorch DataLoaders. We will use a batch size of 256 to maximize GPU throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c41fb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ Downloading Data...\n",
      "âœ… Data Loaded. Train size: 55000 Val size: 5000\n"
     ]
    }
   ],
   "source": [
    "# CELL | PURPOSE: Prepare DataLoaders\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "# Configuration\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Download MNIST\n",
    "print(\"ğŸ“¦ Downloading Data...\")\n",
    "transform = transforms.ToTensor()\n",
    "dataset = MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "mnist_test = MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "# Split Train/Val\n",
    "mnist_train, mnist_val = random_split(dataset, [55000, 5000])\n",
    "\n",
    "# Create Loaders\n",
    "train_loader = DataLoader(mnist_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(mnist_val, batch_size=BATCH_SIZE, num_workers=2)\n",
    "test_loader = DataLoader(mnist_test, batch_size=BATCH_SIZE, num_workers=2)\n",
    "\n",
    "print(\"âœ… Data Loaded. Train size:\", len(mnist_train), \"Val size:\", len(mnist_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be41b368",
   "metadata": {},
   "source": [
    "### **Training with W&B**\n",
    "Now we invoke the **Trainer**. This object handles GPU placement, checkpointing, and logging automatically.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>ğŸ›‘ Checkpoint</b> Run the cell below. You should see a progress bar and a link to your W&B dashboard. Click the link to see your loss curves in real-time!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e25a980",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to render HTML, can't import display from ipython.core\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to render HTML, can't import display from ipython.core\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to render HTML, can't import display from ipython.core\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to render HTML, can't import display from ipython.core\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to render HTML, can't import display from ipython.core\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to render HTML, can't import display from ipython.core\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”³â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type       </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> FLOPs </span>â”ƒ\n",
       "â”¡â”â”â”â•‡â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”©\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>â”‚ net  â”‚ Sequential â”‚  101 K â”‚ train â”‚     0 â”‚\n",
       "â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”³â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mName\u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mType      \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mFLOPs\u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â•‡â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”©\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0mâ”‚ net  â”‚ Sequential â”‚  101 K â”‚ train â”‚     0 â”‚\n",
       "â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 101 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 101 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 5                                                                                           \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total FLOPs</span>: 0                                                                                                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 101 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 101 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 5                                                                                           \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal FLOPs\u001b[0m: 0                                                                                                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ed9206755a442218a82de99d0370bd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CELL | PURPOSE: Run Training\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "# 1. Init Logger\n",
    "wandb_logger = WandbLogger(project=\"DL_Lab_0_Intro\", name=\"MNIST_MLP_Run\")\n",
    "\n",
    "# 2. Init Model\n",
    "model = MNISTClassifier(hidden_dim=128, learning_rate=0.001)\n",
    "\n",
    "# 3. Init Trainer\n",
    "# accelerator=\"auto\" automatically finds the T4 GPU.\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=5,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    logger=wandb_logger,\n",
    "    enable_progress_bar=True\n",
    ")\n",
    "\n",
    "# 4. Train\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c0d980",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>ğŸ“ Reflection</b> \n",
    "\n",
    "<i>Look at the training curves on Weight and Biases and briefly comment in the following.</i>\n",
    "\n",
    "Your Text\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddec2ee",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>ğŸ Milestone</b> You successfully completed training!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6216abee",
   "metadata": {},
   "source": [
    "## **Part 3: The Heavy Artillery (The Cluster)**\n",
    "\n",
    "While Colab is excellent for debugging and small-scale experiments, your major projects may require the University Cluster (`gcluster`).\n",
    "\n",
    "> NOTE: You do not need to run this section today. This is your reference guide for later in the semester.\n",
    "\n",
    "You can find the relevant information in the documentation: https://gcluster.dmi.unict.it/docs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
